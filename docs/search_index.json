[
["index.html", "A Practical Extension of Introductory Statistics in Psychology using R (Draft) This book aims to provide a practical extension of introductory statistics typically taught in psychology into the general linear model (GLM) using R. Chapter 1 Introduction 1.1 What exactly is the GLM? 1.2 How will this book be covered? 1.3 What won’t this book cover? 1.4 Why R?", " A Practical Extension of Introductory Statistics in Psychology using R (Draft) This book aims to provide a practical extension of introductory statistics typically taught in psychology into the general linear model (GLM) using R. Ekarin Eric Pongpipat, M.A. &amp; Giuseppe Miranda, M.S. Chapter 1 Introduction Typically, introductory univariate statistics courses in psychology cover the following inferential analyses (plus or minus a few more analyses): One Sample t-test Dependent Samples t-test Independent Samples t-test One-Way Analysis of Variance (ANOVA) Factorial ANOVA Correlation Simple Linear Regression These conventions may be useful for quickly talking about a particular statistical analysis with others; however, thinking of these analyses as derivatives (or special cases) of the GLM (i.e., ordinary least squares [OLS] regression) lends itself to understanding more advanced statistical techniques. Given that, the book will provide some evidence along with R code for others to see how the aforementioned analyses can be analyzed within the GLM framework with identical answers. The GLM is not a new idea, but an idea that needs emphasizing. Ideally, this book is meant to be an live open-source book that can be edited by others in perpetuity by creating pull requests on github. 1.1 What exactly is the GLM? The general linear model is a unified statistical framework that allows us to think about all of the above analyses (and much more) with a single concise formula: \\[Y = \\beta X + \\varepsilon\\] where \\(Y\\) represents a dependent variable (DV) or a set of DVs, \\(\\beta\\) represents a set of regression coefficients in addition to the constant, \\(X\\) represents an independent variable (IV) or a set of IVs, and \\(\\varepsilon\\) represents the error around the model. This should look familiar as the formula is similar to the simple linear regression formula or the slope-intercept form learned in algebra. 1.2 How will this book be covered? We will go over each of the typical introductory statistics taught in psychology in five steps: State the null and research hypotheses Perform the statistical analysis in R Statistical decision (using an alpha (\\(\\alpha\\)) = 0.05, two-tailed, which is the arbitrary and ubiquitous convention in psychology) APA statement (bare minimum) Visualization For each analysis, the traditional approach will be performed first followed by the GLM approach for steps 1 and 2. The goal is to show the similarities and differences between stating the null and research hypotheses in step 1 as well as how the analyses are identical using both approaches in step 2. 1.3 What won’t this book cover? This book won’t go into assumptions of the inferential tests or exhaustively its respective formulas. This book will also not exhaustively review data manipulation, transformations, and visualization in R as there are other books that already do this well (e.g., R for Data Science by Wickham &amp; Grolemund). 1.4 Why R? We chose to use R to analyze and write this book because R forces us to write out how we performed our analyses (and write helpful comments if we’re nice). These codes allow us and others to re-analyze the data exactly as we have. This is not always the case with statistical software that use GUIs. "],
["prerequisites.html", "Chapter 2 Prerequisites 2.1 Knowledge 2.2 Software", " Chapter 2 Prerequisites 2.1 Knowledge This book assumes that readers have taken or have an understanding of introductory univariate statistics in psychology. We will only briefly cover and review some concepts, but the emphasis will be on thinking about these statistics within the perspective of the GLM framework and how to apply it in R. Thus, readers who do not have this knowledge may find this book useful. A fantastic book to learn the fundamentals of statistics is Data Analysis by Judd, McClelland, &amp; Ryan. 2.2 Software We will also need R (the programming language) and R Studio (an integrated development environment [IDE] for R). "],
["r-packages.html", "Chapter 3 R Packages 3.1 What are packages? 3.2 Packages 3.3 Install R Packages 3.4 Load R Packages", " Chapter 3 R Packages Before we get started, let’s make sure to install and load some necessary R packages. 3.1 What are packages? Packages are an organized set of scripts or code that an author has compiled together. 3.2 Packages The main packages that we will use throughout the book are: psych : Descriptive statistics car : Analysis of Variance (ANOVA) source table with sums of squares type III tidyverse : Data manipulation and visualization knitr : Nice, clean-looking tables carData : Open access datasets MASS : Additional open access datasets scales: Convert numbers into dollars Note: The description of each package is only a description of how we will use each package. Each package can perform many more functions than described above. We recommend briefly reading them by typing ??name of package to read each package’s help page (e.g., ??psych). 3.3 Install R Packages To install a package, we can enter the command install.packages(&quot;name of package&quot;). For example, to install the psych package, we would type install.packages(&quot;psych&quot;). install.packages(&quot;psych&quot;) install.packages(&quot;car&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;knitr&quot;) install.packages(&quot;kableExtra&quot;) install.packages(&quot;carData&quot;) install.packages(&quot;MASS&quot;) install.packages(&quot;scales&quot;) 3.4 Load R Packages To load a package, we can enter the command library(name of package). For example, to load the psych package, we would type library(psych). We won’t be loading the carData and MASS package as we will be extracting the datasets directly from those packages, which we will go over in the datasets chapter. library(psych) library(car) library(tidyverse) library(knitr) library(kableExtra) library(scales) "],
["datasets.html", "Chapter 4 Datasets 4.1 Salaries Dataset 4.2 Anorexia Dataset 4.3 Use Your Own Dataset", " Chapter 4 Datasets Below are two public (open-access) and real datasets that we will use for the analyses. For each dataset, you will find its description, minor data wrangling (or data manipulation), and descriptive statistics in both numeric and visual formats. 4.1 Salaries Dataset datasetSalaries &lt;- carData::Salaries One of the datasets that we’ll use is the Salaries dataset within the carData package. The dataset consists of nine-month salaries collected from 397 collegiate professors in the U.S. during 2008 to 2009. In addition to salaries, the professor’s rank, sex, discipline, years since Ph.D., and years of service was also collected. Thus, there is a total of 6 variables, which are described below. Variable Variable Type Description rank Categorical Professor’s rank of either assistant professor, associate professor, or professor discipline Categorical Type of department the professor works in, either applied or theoretical yrs.since.phd Continuous Number of years since the professor has obtained their PhD yrs.service Continuous Number of years the professor has served the department and/or university sex Categorical Professor’s sex of either male or female salary Continuous Professor’s nine-month salary (USD) 4.1.1 Data Wrangling Before running these analyses within the GLM context, let’s clean up the dataset so that it’s readable for us rather than computers. # spell out rank variables # rename discipline variables to its meaningful name # ensure both rank and discipline are factors datasetSalaries &lt;- datasetSalaries %&gt;% mutate( rank = case_when( rank == &quot;AssocProf&quot; ~ &quot;Associate Professor&quot;, rank == &quot;AsstProf&quot; ~ &quot;Assistant Professor&quot;, rank == &quot;Prof&quot; ~ &quot;Professor&quot; ), discipline = case_when( discipline == &quot;A&quot; ~ &quot;Theoretical&quot;, discipline == &quot;B&quot; ~ &quot;Applied&quot; ) ) %&gt;% mutate( rank = as.factor(rank), discipline = as.factor(discipline) ) 4.1.2 Descriptive Statistics It’s also always a good idea to examine the data numerically and visually. Let’s first look at the categorical variables then the continuous variables. 4.1.2.1 Categorical Variables plyr::count(datasetSalaries, &quot;rank&quot;) ## rank freq ## 1 Assistant Professor 67 ## 2 Associate Professor 64 ## 3 Professor 266 ggplot(datasetSalaries, aes(rank)) + geom_bar(aes(y = stat(count) / sum(stat(count))), color = &quot;black&quot;, fill = &quot;#3182bd&quot;) + theme_classic() + labs(y = &quot;Proportion&quot;, x = &quot;&quot;, title = &quot;rank&quot;) In this dataset, there are a lot more professors than assistant and associate professors combined. plyr::count(datasetSalaries, &quot;discipline&quot;) ## discipline freq ## 1 Applied 216 ## 2 Theoretical 181 ggplot(datasetSalaries, aes(discipline)) + geom_bar(aes(y = stat(count) / sum(stat(count))), color = &quot;black&quot;, fill = &quot;#3182bd&quot;) + theme_classic() + labs(y = &quot;Proportion&quot;, x = &quot;&quot;, title = &quot;discipline&quot;) There are slightly more professors within the applied than the theoretical discipline (i.e., 35 more). plyr::count(datasetSalaries, &quot;sex&quot;) ## sex freq ## 1 Female 39 ## 2 Male 358 ggplot(datasetSalaries, aes(sex)) + geom_bar(aes(y = stat(count) / sum(stat(count))), color = &quot;black&quot;, fill = &quot;#3182bd&quot;) + theme_classic() + labs(y = &quot;Proportion&quot;, x = &quot;&quot;, title = &quot;sex&quot;) There is a little over 9x as many male professors as there are female professors. 4.1.2.2 Continuous Variables datasetSalaries %&gt;% select(yrs.since.phd, yrs.service, salary) %&gt;% describe(.) %&gt;% select(-n, -vars, -trimmed, -mad, -range) %&gt;% round(., 2) ## mean sd median min max skew kurtosis se ## yrs.since.phd 22.31 12.89 21 1 56 0.30 -0.81 0.65 ## yrs.service 17.61 13.01 16 0 60 0.65 -0.34 0.65 ## salary 113706.46 30289.04 107300 57800 231545 0.71 0.18 1520.16 ggplot(datasetSalaries, aes(yrs.since.phd)) + geom_vline( xintercept = describe(datasetSalaries)[&quot;yrs.since.phd&quot;, &quot;mean&quot;], alpha = .5, linetype = &quot;dashed&quot; ) + geom_dotplot(binwidth = 1, fill = &quot;#3182bd&quot;) + geom_text(aes( x = describe(datasetSalaries)[&quot;yrs.since.phd&quot;, &quot;mean&quot;], label = paste(&quot;M =&quot;, round(describe(datasetSalaries)[&quot;yrs.since.phd&quot;, &quot;mean&quot;], 2)), y = -.05 ), angle = 0) + theme_classic() + scale_y_continuous(NULL, breaks = NULL) + labs(x = &quot;&quot;, y = &quot;Frequency&quot;, title = &quot;Years since PhD&quot;) On average, professors have had their Ph.D. for about 22 years. ggplot(datasetSalaries, aes(yrs.service)) + geom_vline( xintercept = describe(datasetSalaries)[&quot;yrs.service&quot;, &quot;mean&quot;], alpha = .5, linetype = &quot;dashed&quot; ) + geom_dotplot(binwidth = 1, fill = &quot;#3182bd&quot;) + geom_text(aes( x = describe(datasetSalaries)[&quot;yrs.service&quot;, &quot;mean&quot;], label = paste(&quot;M =&quot;, round(describe(datasetSalaries)[&quot;yrs.service&quot;, &quot;mean&quot;], 2)), y = -.05 ), angle = 0) + theme_classic() + scale_y_continuous(NULL, breaks = NULL) + labs(x = &quot;&quot;, y = &quot;Frequency&quot;, title = &quot;Years of service&quot;) On average, professors have provided a service to either the department or university for about 17 years and 7 months. ggplot(datasetSalaries, aes(salary)) + geom_vline( xintercept = describe(datasetSalaries)[&quot;salary&quot;, &quot;mean&quot;], alpha = .5, linetype = &quot;dashed&quot; ) + geom_dotplot(binwidth = 3000, fill = &quot;#3182bd&quot;) + geom_text(aes( x = describe(datasetSalaries)[&quot;salary&quot;, &quot;mean&quot;], label = paste(&quot;M =&quot;, scales::dollar(round(describe(datasetSalaries)[&quot;salary&quot;, &quot;mean&quot;], 2))), y = -.05 ), angle = 0) + theme_classic() + scale_y_continuous(NULL, breaks = NULL) + scale_x_continuous(labels = scales::dollar) + labs(x = &quot;&quot;, y = &quot;Frequency&quot;, title = &quot;salary&quot;) On average, a professor’s 9-month annual income is $113,706.46. 4.2 Anorexia Dataset datasetAnorexia &lt;- MASS::anorexia Another dataset that we’ll use is the anorexia dataset within the MASS package. The dataset consists of the weight (in lbs.) of 72 female patients with anorexia before and after either cognitive behavioral therapy, family therapy, or no therapy (control condition). Variable Variable Type Description Treatment Categorical Treatment of female patient with anorexia, either cognitive behavioral therapy (CBT), family therapy (FT), or no therepy (CONT) PreWeight Continuous Weight of female patient with anorexia before treatment in lbs. PostWeight Continuous Weight of female patient with anorexia after treatment in lbs. 4.2.1 Data Wrangling Again, we can make the dataset slightly more readable for us. # spell out variable names # re-order levels to be CBT, FT, then Cont rather than alphabetical order datasetAnorexia &lt;- datasetAnorexia %&gt;% mutate( Treatment = Treat, PreWeight = Prewt, PostWeight = Postwt, Treatment = case_when( Treatment == &quot;Cont&quot; ~ &quot;Control&quot;, TRUE ~ as.character(Treatment) ), Treatment = factor(Treatment, levels = c(&quot;CBT&quot;, &quot;FT&quot;, &quot;Control&quot;)) ) 4.2.2 Descriptive Statistics 4.2.2.1 Categorical Variables plyr::count(datasetAnorexia, &quot;Treatment&quot;) ## Treatment freq ## 1 CBT 29 ## 2 FT 17 ## 3 Control 26 ggplot(datasetAnorexia, aes(Treatment)) + geom_bar(aes(y = stat(count) / sum(stat(count))), color = &quot;black&quot;, fill = &quot;#3182bd&quot;) + theme_classic() + labs(y = &quot;Proportion&quot;, x = &quot;&quot;, title = &quot;Treatment&quot;) There are more participants within the CBT condition compared to either the FT condition or control group. 4.2.2.2 Continuous Variables datasetAnorexia %&gt;% select(PreWeight, PostWeight) %&gt;% describe(.) %&gt;% select(-vars, -trimmed, -mad, -range) %&gt;% round(., 2) ## n mean sd median min max skew kurtosis se ## PreWeight 72 82.41 5.18 82.30 70.0 94.9 -0.05 -0.16 0.61 ## PostWeight 72 85.17 8.04 84.05 71.3 103.6 0.36 -0.81 0.95 ggplot(datasetAnorexia, aes(PreWeight)) + geom_vline( xintercept = describe(datasetAnorexia)[&quot;PreWeight&quot;, &quot;mean&quot;], alpha = .5, linetype = &quot;dashed&quot; ) + geom_dotplot(binwidth = 1, fill = &quot;#3182bd&quot;) + geom_text(aes( x = describe(datasetAnorexia)[&quot;PreWeight&quot;, &quot;mean&quot;], label = paste(&quot;M =&quot;, round(describe(datasetAnorexia)[&quot;PreWeight&quot;, &quot;mean&quot;], 2)), y = -.05 ), angle = 0 ) + theme_classic() + scale_y_continuous(NULL, breaks = NULL) + labs(x = &quot;&quot;, y = &quot;Frequency&quot;, title = &quot;Weight Before Study (lbs)&quot;) On average, women with anorexia before treatment weighed 82.41 lbs. ggplot(datasetAnorexia, aes(PostWeight)) + geom_vline( xintercept = describe(datasetAnorexia)[&quot;PostWeight&quot;, &quot;mean&quot;], alpha = .5, linetype = &quot;dashed&quot; ) + geom_dotplot(binwidth = 1, fill = &quot;#3182bd&quot;) + geom_text(aes( x = describe(datasetAnorexia)[&quot;PostWeight&quot;, &quot;mean&quot;], label = paste(&quot;M =&quot;, round(describe(datasetAnorexia)[&quot;PostWeight&quot;, &quot;mean&quot;], 2)), y = -.05 ), angle = 0 ) + theme_classic() + scale_y_continuous(NULL, breaks = NULL) + labs(x = &quot;&quot;, y = &quot;Frequency&quot;, title = &quot;Weight After Study (lbs)&quot;) On average, women with anorexia after treatment weighed 85.17 lbs, which is about a 2.76 lbs weight gain compared to before treatment. 4.3 Use Your Own Dataset However, if you have an interesting dataset of your own, we encourage you to also try using that dataset alongside ours. "],
["simple-linear-regression.html", "Chapter 5 Simple Linear Regression 5.1 Null and research hypotheses 5.2 Statistical analyses 5.3 Statistical decision 5.4 APA statement 5.5 Visualization", " Chapter 5 Simple Linear Regression The first analysis that we will go over is the simple linear regression. Typically, this is one of the last analyses that is taught in introductory statistics in psychology; however, we will go over the simple linear regression first because its steps are essentially identical to the GLM (i.e., OLS regression) and provides a foundation for the GLM framework. The simple linear regression is typically taught to be used in understanding the relationship between two continuous variables, especially if one can predict the other. However, the simple linear regression is a specific case of regression1 where variables can actually be either continuous or categorical, the number of predictor variables can be more than one, and causation is not a requirement. Thus, regression is applicable to most data types. The simple linear regression is written as: \\[Y = \\beta_0 + \\beta_1X + \\varepsilon\\] where \\(Y\\) is the dependent (or response) variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, \\(X\\) is the independent (or predictor) variable, and \\(\\varepsilon\\) is the error. We will use the terms dependent variable (DV) and response variable interchangeably throughout the book. We will also use the terms independent variable (IV) or predictor variable interchangeably. \\(\\beta\\) represents the true population value, while \\(b\\) represents the unstandardized estimate of the true population value that we will obtain from our analysis. For example, \\(\\beta_1\\) represents the true population slope, while \\(b_1\\) represents the unstandardized estimate of the true population slope (or sample). Thus, a predicted model (as opposed to the above true hypothesized model) would be written as: \\[\\hat{Y} = b_0 + b_1X\\] where \\(\\hat{Y}\\) represents the predicted dependent (or response) variable. Both equations should look familiar to the slope-intercept form learned in algebra: \\[y = mx + b\\] where \\(b\\) is the intercept, and \\(m\\) is the slope. The intercept and slope have the same meaning as they did in algebra. The intercept represents the value of \\(Y\\) when \\(X\\) is 0, and the slope represents the change in \\(Y\\) (\\(\\Delta Y\\)) over the change in \\(X\\) (\\(\\Delta X\\)). In other words, the intercept represents the value of the DV when the IV is 0, and the slope represents the change in the DV over the change in the IV. Note that the \\(b\\) in the slope-intercept form is not the same and should not be confused with the \\(b\\) in the simple linear regression. The \\(b\\) in the simple linear regression is the slope, while the \\(b\\) in the slope-intercept form is the intercept. Unfortunately, using the same symbol for different terms and using different symbols for the same term is prevalent in statistics. For a practical example of the simple linear regression, let’s say that we were interested in examining if there is a relationship between the amount of years since a professor has earned their Ph.D. and their salary. For this example, we will use the datasetSalaries dataset. 5.1 Null and research hypotheses \\[Model: Salary = \\beta_0 + \\beta_1 YearsSincePhD + \\varepsilon\\] \\[H_0: \\beta_1 = 0\\] \\[H_1: \\beta_1 \\ne 0\\] where \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, \\(H_0\\) is the null hypothesis and \\(H_1\\) is the research or alternative hypothesis. Typically, the model is written as the hypothesized true model and thus the use of \\(\\beta\\) rather than \\(b\\). In this example, the null hypothesis states that the slope is equal to 0, or that there is no relationship between \\(X\\) and \\(Y\\). Specifically, in this example, there is no relationship between years since a professor has earned their Ph.D. and their 9-month academic salary. (Note: The null hypothesis is sometimes referred to as the nil hypothesis in psychology since we are typically comparing our estimate against 0, or nil). The research hypothesis states that the slope is not equal to 0, or that there is a relationship between \\(X\\) and \\(Y\\). Specifically, in this example, there is a relationship between years since a professor has earned their Ph.D. and their salary. The model, null hypothesis, and research hypothesis are identical for both traditional and GLM approaches. The intercept can also be tested and its null and research hypotheses are: \\[H_0: \\beta_0 = 0\\] \\[H_1: \\beta_0 \\ne 0\\] The null hypothesis states that the intercept is equal to 0. In other words, the value of \\(Y\\) when \\(X\\) is 0 is not different than 0. Specifically, in this example, the salary of professors who have earned their Ph.D. for less than a year is essentially equal to $0. (Note: Zero represents professors who have just earned their Ph.D. up to those who have earned their Ph.D. for less than a year because yrs.since.phd was collected as integers and anything less than a year would be coded as 0.) The research hypothesis states that the intercept is not equal to 0. In other words, the value of \\(Y\\) when \\(X\\) is 0 is different than 0. Specifically, in this example, the salary of professors who have earned their Ph.D. for less than a year is essentially different than (or not equal to) $0. Again, the null and research hypotheses are identical for both traditional and GLM approaches. 5.2 Statistical analyses To perform the simple linear regression in R, we can use the lm function. model &lt;- lm(formula = salary ~ yrs.since.phd, data = datasetSalaries) The first input is the formula of the model, which is the DV, followed by a tilde sign (~), followed by the IV. In our case, the DV of salary is be the first variable, followed by a ~ and the IV of yrs.since.phd. Notice the similarity between the model and formula. The second input is the dataset (i.e., datasetSalaries). We will save the analysis into an object named model. model can be any name and was chosen for simplicity. We can obtain the ANOVA source table by using the Anova function within the cars package. Anova(model, type = 3) ## Anova Table (Type III tests) ## ## Response: salary ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 8.3369e+11 1 1099.706 &lt; 2.2e-16 *** ## yrs.since.phd 6.3852e+10 1 84.226 &lt; 2.2e-16 *** ## Residuals 2.9945e+11 395 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note: We prefer the Anova function with a capitol A over the anova function with a lowercase a because we can specify the type of sum of squares. Breifly, The sum of squares in the anova function is type I sums of squares and cannot be changed while we can specify type III sums of squares using the Anova function. In short, type I sums of squares performs a sequential test while type III sums of squares performs a simultaneous test. In other words, type I sums of squares partials out shared infomation between the IVs sequentially, while type III sums of squares partials out information shared between each IV from each other simulataneously from the start. The results from using type I and type III sums of squares will be the same if there is only one IV. However, if there is more than one IV, only the last IV will be the same using either type I and type III sums of squares since all information from prior IVs will be partialled out. 5.2.1 How do we read the ANOVA source table? We can look at the variable of interest in the first column and read across that particular variable’s row for its statistical information. For example, let’s first examine the statistics associated with the variable (Intercept). The sums of squares (Sum Sq, which is also abbreviated as \\(SS\\)) associated with the (Intercept) is 8.37e+11, the degrees of freedom (Df, specifically, degrees of freedom reduced) is 1, the F-statistic (F value) is 1099.71, and the p-value (Pr&gt;F) is &lt; 2.2e-16. The F value or F-statistic conceptually provides a ratio of explained to unexplained variability. Explained variability is how much the DV can be explained by IVs while unexplained variability is how much variability is not explained by the IV. The mathematical formula of the F-statistic can conceptually be thought of as: \\[F = \\frac{explained\\ variability}{unexplained\\ variability}\\] Thus, if the IV can’t explain any of the variability within the DV, then the F-statistic is less than or equal to 1. However, if the IV(s) can explain the DV, the F-statistic will be larger than 1. In other words, the higher the F-statistic, the more the IV(s) explain(s) the variability in the DV compared to its inability to explain the DV. Thus, the interpretation of the F-statistic of the (Intercept) is that the (Intercept) can explain the variability in salary 1099.71 times more than the (Intercept)’s inability to explain the variability in salary.2 The Pr(&gt;F) or p-value associated with the Intercept is &lt; 2.2e-16. The p-value is the probability of obtaining that particular F-statistic or more extreme assuming that the null hypothesis is true. In other words, the probability of obtaining this F-statistic by chance is extremely small. Since yrs.since.phd was our main predictor of interest (and for extra practice), let’s also look at the statistics associated with yrs.since.phd. The F-statistic associated with yrs.since.phd is 84.23, and can be interpreted as yrs.since.phd explains the variability in salary 84.23 times more than years.since.phd’s inability to expain the variability in salary. The mathematical formula and calculation for yrs.since.phd can also be seen in the footnotes section.3 The p-value associated with yrs.since.phd is also &lt; 2.2e-16 with identical interpretations for the p-value of the (Intercept). We can also obtain the coefficients table by using the summary function. summary(model) ## ## Call: ## lm(formula = salary ~ yrs.since.phd, data = datasetSalaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -84171 -19432 -2858 16086 102383 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 91718.7 2765.8 33.162 &lt;2e-16 *** ## yrs.since.phd 985.3 107.4 9.177 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 27530 on 395 degrees of freedom ## Multiple R-squared: 0.1758, Adjusted R-squared: 0.1737 ## F-statistic: 84.23 on 1 and 395 DF, p-value: &lt; 2.2e-16 5.2.2 How do we read the coefficients table? Similarly, we can look at the variable of interest in the first column and read across that particular variable’s row for its statistical information. Again, let’s first take a look at the statistics associated with the (Intercept). The Estimate of the (Intercept) is 91718.7, the standard error (Std. Error) is 2765.8, the t-statistic (t value) is 33.162, and the p-value ((Pr&gt;|t|)) is &lt; 2e-16. The Estimate of the (Intercept) is the predicted intercept value, \\(b_0 = 91718.7\\). Thus, the predicted salary when \\(yrs.since.phd = 0\\) is $91,718.70. In other words, when a professor has had their Ph.D. for less than a year, their salary is predicted to be $91,718.70. The t value or t-statistic is similar to the F-statistic, and can also be conceptually thought of as the ratio of systematic differences to unsystematic differences. The t-statistic is equal to the square root of the F-statistic when the degrees of freedom is 1.45 Thus, the t-statistic interpretation is that the (Intercept) explains the variability in salary 33.162 times more than its inability to explain salary. The p-value or Pr(&gt;|t|) of &lt;2e-16 is the probability of obtaining that particular t-statistic or a value more extreme than the t-statistic assuming that the null hypothesis is true, which is the same value from the ANOVA source table. Let’s also examine statistics for yrs.since.phd. The Estimate of yrs.since.phd is 985.3, the Std. Error is 107.4, the t value is 9.18, and the Pr(&gt;|t|) is &lt;2e-16. The Estimate of the yrs.since.phd is the estimated slope, \\(b_1 = 985.3\\). Thus, salary is predicted to change by $985.30 for every 1 unit increase in yrs.since.phd.6 In other words, salary is predicted to increase by $985.30 for every additional year a professor has earned their Ph.D. The t-statistic can be interpreted as yrs.since.phd explains the variability in salary 9.18 times more than yrs.since.phd’s inability to the explain the variability in salary. The p-value of yrs.since.phd is identical to the p-value of the (Intercept) and has identical interpretations. Now, if wanted, we can fill in our predicted model, \\(\\hat{Y} = b_0 + b_1X\\), with the estimates from the coefficients table to predict (or estimate) other values of salary given a certain number of years since a professor has earned their Ph.D. \\[\\hat{salary} = b_0 + b_1*yrs.since.phd = 91718.7+985.3*yrs.since.phd\\] For example, let’s say that we were interested in estimated the salary of professor’s who have worked for 2 years. \\[\\hat{salary} = 91718.7+985.3*yrs.since.phd = 91718.7+985.3*2 = 93689.3\\] 5.3 Statistical decision To determine if yrs.since.phd is statistically significant in predicting salary, we will compare its p-value with the alpha level (\\(\\alpha\\)) of 0.05. \\(\\alpha\\) is the Type I error rate, or the probability that we are incorrectly rejecting the null hypothesis when it is actually true. Given that the p-value of &lt;2.2e-16 for the yrs.since.phd is less than the alpha level (\\(\\alpha\\)) of 0.05, we will reject the null hypothesis. We can also determine if the intercept is statistically significant. Given, that the (Intercept) had an identical p-value, we will also reject the null hypothesis. 5.4 APA statement A regression was performed to test if the amount of years since obtaining a Ph.D. was related to their salary. The amount of years since obtaining a Ph.D. was significantly related to their salary, b = 985.3, t(395) = 9.18, p &lt; .001. For every passing year that a professor has earned their Ph.D., their salary is estimated to increase by $985.30. 5.5 Visualization ggplot(datasetSalaries, aes(yrs.since.phd, salary)) + geom_point(alpha = 0.25) + geom_smooth(method = &quot;lm&quot;, color = &quot;#3182bd&quot;) + labs( x = &quot;Years since Ph.D.&quot;, y = &quot;9-Month Salary (USD)&quot; ) + scale_y_continuous(labels = dollar) + theme_classic() Figure 5.1: A scatterplot of years since Ph.D. and salary along with the line of best fit. The gray band represents the 95% confidence interval (CI). \\(m_{rank}=k_{rank}-1=3-1=2\\)↩ \\(m_{rank}=k_{rank}-1=3-1=2\\)↩ \\(\\Sigma Contrast\\_Codes_{TenuredvAssistant} = -2 + 1 + 1 = 0\\)↩ \\(\\Sigma Contrast\\_Code_{AssociatevProfessor} = 0 -1 + 1 = 0\\)↩ \\(\\Sigma Contrast_{TenuredvAssistant}*Contrast_{AssociatevProfessor} = (2*0) + (1*-1) + (1*1) = 0\\)↩ \\[b_1 = \\frac{\\Delta Y}{\\Delta X} = \\frac{\\Delta DV}{\\Delta IV} = \\frac{\\Delta salary}{\\Delta yrs.since.phd} = \\frac{985.3}{1} = 985.3\\]↩ "],
["correlation.html", "Chapter 6 Correlation 6.1 Null and research hypotheses 6.2 Statistical analysis 6.3 Statistical decision 6.4 APA statement 6.5 Visualization", " Chapter 6 Correlation The next analysis that we will go over is the correlation. The correlation and simple linear regression are similar with the only difference being that the estimate of the slope of the correlation is standardized, while the estimate of the slope of the simple linear regression is unstandardized. The correlation that is typically taught is the Pearson correlation, which is the correlation between two continuous variables. However, despite the difference in correlation nomenclature for different variable types, all correlations are similarly applied in the GLM approach. As a side note, this is also the chapter that people associate the phrase “correlation is not causation.” This phrase is correct and should be thoughtfully considered; however, all statistical analyses are looking at how variables relate (or correlate or covary) with each other. Thus, the phrase “correlation is not causation” is about research design rather than statistical analysis. Given that the simple linear regression and correlation are similar, we can use the same example from the previous chapter, where we were interested in determining if there was a relationship (or correlation) between the number of years since a professor has had their Ph.D. and their salary. We will be using the same datasetSalaries dataset to facilitate comparisons. 6.1 Null and research hypotheses 6.1.1 Traditional approach \\[H_0: \\rho = 0\\] \\[H_1: \\rho \\ne 0\\] where \\(\\rho\\) represents the population correlation value (Note: \\(r\\) represents the estimated population correlation value or sample correlation value.) The null hypothesis states that there is no correlation between the amount of years since earning a Ph.D. and their salary (i.e., their relationship is equal to 0). In contrast, the research hypothesis states there is a correlation (i.e., the relationship is not equal to 0). 6.1.2 GLM approach \\[Model: Z_{salary} = \\beta_0 + \\beta_1*Z_{years.since.phd} + \\varepsilon\\] \\[H_0: \\beta_1 = 0\\] \\[H_1: \\beta_1 \\ne 0\\] where Z is the standardized (i.e., z-scored) version of that particular variable. As a reminder, z-scoring converts a variable to have a mean of 0 and a standard deviation of 1, but its distribution is unchanged. In this example, the correlation value (\\(r\\)) is simply a standardized slope (\\(\\beta\\)). Thus, standardizing both the DV and IV standardizes the slope. In other words, \\(r\\) is the standardized form of \\(b\\). (Note: The estimated standardized slope within the regression context is unfortunately referred to by the same symbol as the true population regression coefficient symbol, \\(\\beta\\).) 6.2 Statistical analysis 6.2.1 Traditional approach Using the traditional approach, we can test the correlation by using the cor.test() function. The first input is the IV (yrs.since.phd) and the second input is the DV (salary). However, the order is essentially arbitrary and can be switched. Notice that in each input, we also included the name of the dataset (i.e., datasetSalaries) before each variable, which is seperated by a $. The $ calls an object within a larger object. In this case, the $ calls the object or variable yrs.since.phd within the larger object of the datasetSalaries dataset. cor.test(datasetSalaries$yrs.since.phd, datasetSalaries$salary) ## ## Pearson&#39;s product-moment correlation ## ## data: datasetSalaries$yrs.since.phd and datasetSalaries$salary ## t = 9.1775, df = 395, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3346160 0.4971402 ## sample estimates: ## cor ## 0.4192311 The new statistic that is shown in this analysis is the correlation (cor) and the 95 percent confidence interval. The correlation is the estimated correlation, \\(r\\), which equals 0.42. The 95% confidence interval represents our probabilistic confidence (i.e., 95% confident) that our true population correlation, \\(\\rho\\), would fall between the values of 0.33 and 0.50. 6.2.2 GLM approach To perform a correlation using the GLM approach, we can again use the lm function. model &lt;- lm(scale(salary) ~ 1 + scale(yrs.since.phd), data = datasetSalaries) Remember, both the DV and IV need to be standardized so that our estimates are also standardized. Luckily, we can standardize both salary and yrs.since.phd directly in the formula by using the scale function. The table that most aligns with the correlation output is the coefficients table and not the ANOVA source table, so we will only be using the summary() function for this analysis. However, we could use the Anova() function to obtain the ANOVA source table if desired. summary(model) ## ## Call: ## lm(formula = scale(salary) ~ 1 + scale(yrs.since.phd), data = datasetSalaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7789 -0.6415 -0.0944 0.5311 3.3802 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.886e-17 4.562e-02 0.000 1 ## scale(yrs.since.phd) 4.192e-01 4.568e-02 9.177 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.909 on 395 degrees of freedom ## Multiple R-squared: 0.1758, Adjusted R-squared: 0.1737 ## F-statistic: 84.23 on 1 and 395 DF, p-value: &lt; 2.2e-16 Since the estimates are standardized, the units are in standard deviations rather than their original metric. For example, the slope represents the standard deviation unit change in Y for every one standard deviation unit increase in X. In this specific example, the slope of scale(yrs.since.phd) represents the standard deviation unit change in salary for every one standard deviation unit increase yrs.since.phd. In other words, there is a 0.42 standard deviation unit increase in salary for every one standard deviation unit increase in the number of years a professor has had their Ph.D. The intercept represents Y in standard deviation units when X is 0 standard deviation units, or at the average value of X. Remember, that 0 standard deviations (or a z-score of 0) is also the mean because z-scoring converts the mean to 0 and the standard deviation to 1. Thus, in this example, the intercept represents the predicted salary in standard deviation units when yrs.since.phd is 0 standard deviation units. In other words, for a professor that has had their Ph.D. for 0 standard deviation units (or an average number of years), their predicted salary is -5.89e-17 standard deviation units. Given these interpretations of standardized slopes and intercepts, they are not our preference, especially when variables have easily interpreted metrics. However, standardized slopes and intercepts are preferred for quickly determining effect sizes as they can only range between -1 and +1. They are also preferred when comparing other predictors within and between other models as they are in a standardized metric; however, the validity of these comparisons are debated. confint(model) ## 2.5 % 97.5 % ## (Intercept) -0.08969389 0.08969389 ## scale(yrs.since.phd) 0.32942404 0.50903818 Notice that in both analyses the values are identical with respect to place of rounding. The correlation \\(r\\) value is .42, t-statistic is 9.18 with 395 degrees of freedom (df), and the p-value is 2.2e-16. 6.3 Statistical decision Given the p-value of 2.2e-16 is smaller than the alpha level (\\(\\alpha\\)) of 0.05, we will reject the null hypothesis. 6.4 APA statement A correlation was performed to test whether the amount years since obtaining a Ph.D. is related to salary. The amount of years since obtaining a Ph.D. was significantly related to salary, r(395) = 0.42, p &lt; .001. 6.5 Visualization ggplot(datasetSalaries, aes(scale(yrs.since.phd), scale(salary))) + geom_point(alpha = 0.25) + geom_smooth(method = &quot;lm&quot;, color = &quot;#3182bd&quot;) + labs( x = &quot;Years since Ph.D.&quot;, y = &quot;9-Month Salary (USD)&quot; ) + scale_y_continuous(labels = dollar) + theme_classic() Figure 6.1: A scatterplot of years since Ph.D. and salary along with the line of best fit. The gray band represents the 95% CI. "],
["one-sample-t-test.html", "Chapter 7 One Sample t-Test 7.1 Null and research hypotheses 7.2 Statistical analysis 7.3 Statistical decision 7.4 APA statement 7.5 Visualization", " Chapter 7 One Sample t-Test Now that we have gone over the simple linear regression and how the simple linear regression is written and interpreted identically to the GLM (in addition to its standardized counterpart, the correlation), we can begin covering the other statistical analyses in the order they are typically taught. Let’s now go over the one-sample t-test, which compares a sample mean to a population mean (or an a priori value). For example, let’s say we were interested in determining if the salary of professors was significantly different than the national U.S. median income of $50,221 in 2009. For this example, we will again be using the datasetSalaries dataset. 7.1 Null and research hypotheses 7.1.1 Traditional Approach \\[H_0: \\mu = \\$50,221\\] \\[H_1: \\mu \\ne \\$50,221\\] where \\(\\mu\\) represents the population mean The null hypothesis states that there is no difference between the sample and population mean, or equivalently the sample and population mean are equal. The research hypothesis states that there is a difference between the sample and population mean, or equivalently the sample and population mean are not equal. 7.1.2 GLM Approach \\[Model: Salary = \\beta_0 + \\varepsilon\\] \\[H_0: \\beta_0 = \\$50,221\\] \\[H_1: \\beta_0 \\ne \\$50,221\\] where \\(\\beta_{0}\\) represents the intercept, \\(\\varepsilon\\) represents the population error, \\(H_0\\) represents the null hypothesis, and \\(H_1\\) represents the research hypothesis. In this particular case, we will not be using the nil hypothesis as we have an a priori comparison of $50,221. In the model, when there is no other predictor, the intercept will be the mean. This is because without any other information, the single best number to describe the data is the mean. Thus, the null hypothesis states the intercept (or the mean of 9-month academic salary of professors) is equal to $50,221. The research hypothesis states that the intercept (or the mean 9-month academic salary of professors) is not equal to $50,221. 7.2 Statistical analysis 7.2.1 Traditional Approach To perform the traditional one-sample t-test, we can use the t.test() function. The first input is the DV of salary, which is again prefixed by the name of the dataset and the dollar sign. The second input is the a priori value (or population value, \\(\\mu\\)) that we are interested in testing (i.e., $50,221). t.test(x = datasetSalaries$salary, mu = 50221) ## ## One Sample t-test ## ## data: datasetSalaries$salary ## t = 41.762, df = 396, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 50221 ## 95 percent confidence interval: ## 110717.9 116695.1 ## sample estimates: ## mean of x ## 113706.5 From this output, we can see that the t-statistic (t) is 41.762, degrees of freedom (df) is 396, and the p-value is 2.2e-16, with professor’s mean salary at $113,706.50. Therefore, professors earn a significantly higher salary compared with the national U.S. median income of $50,221 in 2009. The t-statistic and its respective p-value can be interpreted as we previously saw in the simple linear regression chapter. We will spend more time explaining these values in the next section. 7.2.2 GLM Approach Since we have an a priori hypothesis (i.e., median income of $50,221), we will need to first manipulate the DV by point deviating from this a priori value. To do this, we will need to subtract $50,221 from each salary score. Luckily, we can do that directly within the lm() fucntion. We have to point-deviate because the lm() function always tests regression estimates including the intercept against 0 (\\(H_0 = 0\\)). By point deviating from $50,221, we are now asking if the mean of the new point deviated scores is different than 0, where 0 now equals $50,221. To test the intercept, we simply place a 1 as the predictor. model &lt;- lm(salary - 50221 ~ 1, datasetSalaries) We will again only look at the coefficients table using the summary() function for this test and subsequent t-tests. Although, the ANOVA source table is not typically associated with this test, we could always examine it if desired using the Anova() function. summary(model) ## ## Call: ## lm(formula = salary - 50221 ~ 1, data = datasetSalaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -55906 -22706 -6406 20479 117839 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 63486 1520 41.76 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 30290 on 396 degrees of freedom Notice that in both approaches that the values are identical with respect to the place of rounding. The t-statistic is 41.76 with 396 degrees of freedom (df), and the p-value is &lt;2e-16. However, the estimate of the GLM approach is now the sample mean of the point deviated scores. Thus, we if add back $50,221, we will obtain the same sample mean of $113,7077. Let’s go over the t-statistic and p-value to reinforce these concepts. Remember that the t-statistic represents the systematic difference compared to the unsystematic difference. In this case, the systematic difference is the difference between professor’s mean salary and U.S. median income (i.e., $113,707 - $50,221) and the unsystematic difference is standard error of the mean of the sample (in other words, random variability in the sample). Also, the p-value represents the probability of finding a particular t-statistic or t-statistic more extreme assuming that the null hypothesis is true. In this case, the probability of finding a t-statistic of 41.76 or more extreme is &lt;2.2e-16, which is extremely small and not likely to occur by chance. 7.3 Statistical decision Given that the p-value of 2.2e-16 is smaller the alpha level (\\(\\alpha\\)) of 0.05, we will reject the null hypothesis. 7.4 APA statement A one sample t-test was performed to test if the salary of professors was different than the national U.S. median of $50,221. Professor salaries (M = $113,706, SD = $30,289) were significantly higher than the national U.S. median income, t(396) = 41.76, p &lt; .001. 7.5 Visualization # calculate descriptive statistics along with the 95% CI dataset_summary &lt;- datasetSalaries %&gt;% dplyr::summarize( mean = mean(salary), sd = sd(salary), n = n(), sem = sd / sqrt(n), tcrit = abs(qt(0.05 / 2, df = n - 1)), ME = tcrit * sem, LL95CI = mean - ME, UL95CI = mean + ME ) # plot ggplot(datasetSalaries, mapping = aes(&quot;&quot;, salary)) + geom_jitter(alpha = 0.1, width = 0.1) + geom_hline( yintercept = 50221, alpha = .5, linetype = &quot;dashed&quot; ) + geom_errorbar( data = dataset_summary, aes( y = mean, ymin = LL95CI, ymax = UL95CI ), width = 0.01, color = &quot;#3182bd&quot; ) + geom_point( data = dataset_summary, aes(&quot;&quot;, mean), size = 2, color = &quot;#3182bd&quot; ) + labs( x = &quot;0&quot;, y = &quot;9-Month Salary (USD)&quot; ) + theme_classic() + scale_y_continuous(labels = scales::dollar) Figure 7.1: A dot plot of the salary of professors where the dot is the mean salary of professors and the whiskers are the 95% CI. Note: The data points are actually only on a single line on the x-axis. They are only jittered (dispersed) for easier visualization of all data points. \\(m_{rank}=k_{rank}-1=3-1=2\\)↩ "],
["dependent-samples-t-test.html", "Chapter 8 Dependent Samples t-Test 8.1 Null and research hypotheses 8.2 Statistical analysis 8.3 Statistical decision 8.4 APA statement 8.5 Visualization", " Chapter 8 Dependent Samples t-Test The dependent samples t-test is essentially the same analysis as the one sample t-test but on a difference score. For example, let’s say that we were interested in determining if the weight of female patients with anorexia changed before the study compared to after the study. For this example, we will be using the datasetAnorexia dataset. We use a difference score because the measures of weight are dependent on (or relate to) each other as they come from the same individual. In other words, weights from the same individual are more likely to be closer in value than weights between individuals. If we do not take this dependence (i.e., positive dependence) into account, the t-value will be artificially inflated (or higher than it should be). Thus, we are also more likely to artificially reduce the p-value and ultimately commit a Type I error. In other cases, not properly accounting for the dependence of measures can artificially reduce the t-value if the measures are negatively dependent. Measuring responsibility of household chores in couples is an example of negative dependence because as one couple rates their household chore responsbility as high or low, the other couple will typically rate the opposite. In other words, the measures of responsibility of household chores are more likely to be different within couples than the responsibility of household chores between couples. The artificially reduced t-value would increase the p-value and are ultimately likely to commit a Type II error. 8.1 Null and research hypotheses 8.1.1 Traditional approach \\(H_0: \\mu_{WeightDifference} = 0\\) \\(H_1: \\mu_{WeightDifference} \\ne 0\\) The null hypothesis states that there is no weight difference in female patients with anorexia before and after the study. The research hypothesis states there is a weight difference in female patients with anorexia before and after the study. 8.1.2 GLM approach \\(Model: WeightDifference = \\beta_0 + \\varepsilon\\) \\(H_0: \\beta_0 = 0\\) \\(H_1: \\beta_0 \\ne 0\\) where \\(\\beta_0\\) represents the intercept and \\(\\varepsilon\\) represents the error Just like in the one sample t-test, the intercept (\\(\\beta_{0}\\)) represents the sample mean. However, in this case, the sample mean is the mean of the weight difference of the female patients with anorexia before and after the study. Thus, the intercept here is testing if the sample mean of the weight difference score is significantly different than 0 with identical null and research hypotheses. 8.2 Statistical analysis 8.2.1 Traditional approach To perform the traditional dependent samples t-test, we can again use the t.test() function. However, we will also set the paired option to TRUE. t.test(datasetAnorexia$PostWeight, datasetAnorexia$PreWeight, paired = TRUE) ## ## Paired t-test ## ## data: datasetAnorexia$PostWeight and datasetAnorexia$PreWeight ## t = 2.9376, df = 71, p-value = 0.004458 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.8878354 4.6399424 ## sample estimates: ## mean of the differences ## 2.763889 From this output, we can see that the t-statistic (t) is 2.9376, degrees of freedom (df) is 71, and the p-value is 0.004458. 8.2.2 GLM approach For the DV, we will input the difference of weight before and after treatment (i.e., PostWeight-PreWeight) directly into the lm() function. Since we are testing the intercept, we will again place 1 as the predictor. model &lt;- lm(PostWeight - PreWeight ~ 1, datasetAnorexia) summary(model) ## ## Call: ## lm(formula = PostWeight - PreWeight ~ 1, data = datasetAnorexia) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.964 -4.989 -1.114 6.336 18.736 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.7639 0.9409 2.938 0.00446 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.984 on 71 degrees of freedom Notice that in both analyses the sample mean difference of 2.76, the t-statistic of 2.94 with 71 degrees of freedom (df), and the p-value of .004 are identical with respective to place of rounding. In this case, the systematic difference of the t-statistic is the difference in weight before and after a study and the unsystematic difference is standard error of the mean of the differences (in other words, random variability of the difference scores). In this case, the probability of finding a t-statistic of 2.94 or more extreme is .004, which is very small and not likely to occur by chance if the null hypothesis was true. 8.3 Statistical decision Given the p-value of 0.004 is smaller than the alpha level (\\(\\alpha\\)) of 0.05, we will reject the null hypothesis. 8.4 APA statement A dependent samples t-test was performed to test if female patients with anorexia had changed their weight before and after the study. The female patients with anorexia significantly gained weight after the study (M = 85, SD = 8) compared to before the study (M = 82, SD = 5), t(71) = 2.94, p = .004. 8.5 Visualization # calculate descriptive statistics along with the 95% CI dataset_summary &lt;- datasetAnorexia %&gt;% summarize( mean = mean(PostWeight - PreWeight), sd = sd(PostWeight - PreWeight), n = n(), sem = sd / sqrt(n), tcrit = abs(qt(0.05 / 2, df = n - 1)), ME = tcrit * sem, LL95CI = mean - ME, UL95CI = mean + ME ) ggplot(datasetAnorexia, aes(&quot;&quot;, PostWeight - PreWeight)) + geom_hline(yintercept = 0, alpha = .1, linetype = &quot;dashed&quot;) + geom_jitter(alpha = 0.25, width = 0.02) + geom_errorbar(data = dataset_summary, aes(y = mean, ymin = LL95CI, ymax = UL95CI), width = 0.01, color = &quot;#3182bd&quot;) + geom_point(data = dataset_summary, aes(&quot;&quot;, mean), size = 3, color = &quot;#3182bd&quot;) + labs(x = 0, y = &quot;Weight Difference (lbs)&quot;) + theme_classic() Figure 8.1: A dot plot of the weight change of female patients with anorexia before and after the study. The dot is the mean weight change and the bars represent the 95% CI. Note: The data points are actually only on a single line on the x-axis. They are only jittered (dispersed) for easier visualization of all data points. "],
["independent-samples-t-test.html", "Chapter 9 Independent Samples t-Test 9.1 Null and research hypotheses 9.2 Statistical analysis 9.3 Statistical decision 9.4 APA statement 9.5 Visualization", " Chapter 9 Independent Samples t-Test The independent samples t-test compares the means of two different (or independent) samples. For example, let’s say that we were interested in determining if the salary of professors was different depending on whether they were part of an applied or theoretical discipline. For this example, we will use the datasetSalaries data set. 9.1 Null and research hypotheses 9.1.1 Traditional approach \\(H_0: \\mu_{Applied} - \\mu_{Theoretical} = 0\\) or equivalently \\(\\mu_{Applied} = \\mu_{Theoretical}\\) \\(H_1: \\mu_{Applied} - \\mu_{Theoretical} \\ne 0\\) or equivalently \\(\\mu_{Applied} \\ne \\mu_{Theoretical}\\) The null hypothesis states there is no difference in the salary of professors who are in an applied discipline compared to a theoretical discipline. The research hypothesis states there is a difference in the salary of professors who are in an applied discipline compared to a theoretical discipline. 9.1.2 GLM approach \\[Model: Salary = \\beta_0 + \\beta_1*Discipline + \\varepsilon\\] \\[H_0: \\beta_1 = 0\\] \\[H_1: \\beta_1 \\ne 0\\] In addition to the intercept (\\(\\beta_0\\)), we now have a predictor discipline along with its associated slope (\\(\\beta_1\\)). In this model, the slope represents the change in salary over the change in discipline, and the intercept (\\(\\beta_{0}\\)) in represents the value when discipline is 0. The null hypothesis states that the slope associated with discipline is equal to zero. In other words, there is no difference in the salary of professors who are in different disciplines. The alternative hypothesis states that the slope associated with discipline is not equal to zero. In other words, there is a difference in the salary of professors that are in different disciplines. The interpretation of the slope and intercept depends on how discipline is coded. Thus, it is always a good idea to check how this categorical IV is coded, which can be done using the contrasts() function. contrasts(datasetSalaries$discipline) ## Theoretical ## Applied 0 ## Theoretical 1 We can see that Applied is coded as 0 and Theoretical is coded as 1. Given, that the difference of coding scheme of discipline is 1, the slope represents the mean difference in salary for professors that are in theoretical disciplines compared to applied disciplines.8 Additionally, the intercept (\\(\\beta_{0}\\)) represents the mean salary of professors in the applied discipline since 0 represents Applied in the discipline coding scheme. If we wanted to change the coding scheme and code Theoretical as 0 and Applied as 1, then our interpretation of the intercept would be the mean salary of professors in the theoretical discipline since Theoretical is now 0. The slope would still have the same interpretation of the mean difference of salary for professors in different disciplines as the difference in the coding scheme is still 1. However, the sign would change.9 These two types of coding scheme is known as dummy coding, which is R’s default coding scheme for categorical variables. Specifically, dummy coding is when one level of an IV is coded as 1 and all others are coded as 0. However, there are other coding schemes such as effects (also known deviant), helmert, polynomial, and orthogonal. For a good description of different contrasts in addition to applying and interpreting them, check out this website. We will also go over coding categorical variables in more detail in the next chapter. Our preferred contrast for the independent samples t-test is to use -0.5 for one group and 0.5 for the other group. We prefer this coding scheme because the slope will still provide the mean difference;10 however, since 0 lies in between the two groups, the intercept will now represent the mean of the group means. In this example case, it will be the mean of the mean salary of professors in the applied discipline and the mean salary of professors in the theoretical discipline. We recommend assigning the group expected to have a higher value in the dependent variable as 0.5 and the other group as -0.5. In our example, we might expect that those in the applied discipline will have higher salaries and thus assign that group to 0.5, while the theoretical discipline to have lower salaries and thus assign them to -0.5. We can do this by using the concatenate c() function to group the numbers together and assign them back to the contrast. The order inside the c() function must be in alphanumerical order of the levels of the independent variable. contrasts(datasetSalaries$discipline) &lt;- c(0.5, -0.5) contrasts(datasetSalaries$discipline) ## [,1] ## Applied 0.5 ## Theoretical -0.5 Note: If the difference in discipline was not equal to 1, the estimate would equal the fraction of the difference. For example, if Applied was coded as -1 and Theoretical was coded as 1, the difference of discipline is now 2 and the estimate would represent half of the salary mean difference.11 Thus, if we multiplied the estimate by 2, we would obtain the mean salary difference. Even though the estimate changes, the t-statistic and p-value will not change as the intercept and error will adjust proportionally to the coding scheme of the categorical IV (as long as they are unique values). 9.2 Statistical analysis 9.2.1 Traditional approach To perform the traditional independent samples t-test, we can again use the t.test() function. However, we will now enter the formula of the GLM into the first argument. For this test, we will assume the variances of each group are equal (not significantly different from each other); however, this should be tested. t.test(datasetSalaries$salary ~ datasetSalaries$discipline, var.equal = TRUE) ## ## Two Sample t-test ## ## data: datasetSalaries$salary by datasetSalaries$discipline ## t = 3.1406, df = 395, p-value = 0.001813 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 3545.70 15414.83 ## sample estimates: ## mean in group Applied mean in group Theoretical ## 118028.7 108548.4 Note: There are other ways to enter the formula into t.test() function depending on how the dataset is formatted. From this output we can see that the t-statistic (t) is 3.1406, degrees of freedom (df) is 395, and p-value is 0.001813, with the mean salary of professors in the Applied discipline being $118,028.70 and the mean salary of professors in the Theoretical discipline being $108,548.40. Therefore, professors with Applied disciplines earn significantly higher salaries than professors with Theoretical disciplines. 9.2.2 GLM approach model &lt;- lm(salary ~ 1 + discipline, datasetSalaries) summary(model) ## ## Call: ## lm(formula = salary ~ 1 + discipline, data = datasetSalaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -50748 -24611 -4429 19138 113516 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 113289 1509 75.060 &lt; 2e-16 *** ## discipline1 9480 3019 3.141 0.00181 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29960 on 395 degrees of freedom ## Multiple R-squared: 0.02436, Adjusted R-squared: 0.02189 ## F-statistic: 9.863 on 1 and 395 DF, p-value: 0.001813 Notice that in both analyses, the t-statistic (t value) of -3.14 with 395 degrees of freedom (df), and p-value of .002 are identical to the output from the t.test() function. We can also see that if we subtract the mean salary for professors in the theoretical discipline from the applied discipline from the t.test() results, we obtain the same mean difference in the estimate in the GLM results (i.e., $118,028.70 - $108,548.40 = $9,480.30). Furthermore, we can also see that the intercept is the mean of the mean salary from both those in the applied discipline and theoretical discipline (\\(\\frac{118028.70+108548.40}{2} = 113288.50\\)). 9.3 Statistical decision Given the p-value of .002 is less than the alpha level (\\(\\alpha\\)) of 0.05, we will reject the null hypothesis. 9.4 APA statement An independent samples t-test was performed to test if salary of professors was different depending on their discipline. The salary of professors was significantly higher for professors in applied disciplines (M = $118,029, SD = $29,459) than for professors in theoretical disciplines (M = $108,548, SD = $30,538), t(395) = -3.14, p = .002. 9.5 Visualization # calculate descriptive statistics along with the 95% CI dataset_summary &lt;- datasetSalaries %&gt;% mutate(discipline = ifelse(discipline == &quot;Applied&quot;, 0.5, -0.5)) %&gt;% group_by(discipline) %&gt;% summarize( mean = mean(salary), sd = sd(salary), n = n(), sem = sd / sqrt(n), tcrit = abs(qt(0.05 / 2, df = n - 1)), ME = tcrit * sem, LL95CI = mean - ME, UL95CI = mean + ME ) mean_of_means &lt;- mean(dataset_summary$mean) # plot datasetSalaries %&gt;% mutate(discipline = ifelse(discipline == &quot;Applied&quot;, 0.5, -0.5)) %&gt;% ggplot(., aes(discipline, salary)) + geom_jitter(alpha = 0.1, width = 0.05) + geom_line(data = dataset_summary, aes(x = discipline, y = mean), color = &quot;#3182bd&quot;) + geom_errorbar(data = dataset_summary, aes(x = discipline, y = mean, ymin = LL95CI, ymax = UL95CI), width = 0.02, color = &quot;#3182bd&quot;) + geom_point(data = dataset_summary, aes(x = discipline, y = mean), size = 3, color = &quot;#3182bd&quot;) + geom_point(aes(x = 0, y = mean_of_means), size = 3, color = &quot;#3182bd&quot;) + labs( x = &quot;Discipline&quot;, y = &quot;9-Month Academic Salary (USD)&quot;, caption = &quot;&quot; ) + theme_classic() + scale_y_continuous( labels = scales::dollar ) + scale_x_continuous(breaks = c(-1,-.5,0,.5,1)) + annotate(geom = &quot;text&quot;, x = -.5, y = 0, label = &quot;Applied&quot;, size = 4) + annotate(geom = &quot;text&quot;, x = .5, y = 0, label = &quot;Theoretical&quot;, size = 4) Figure 9.1: A dot plot of the 9-month academic salaries of professors that are in applied compared to theoretical disciplines. With respect to each discipline, the dot represents the mean salary and the bars represent the 95% CI. Note: The data points are actually only on a single line on the x-axis. They are only jittered (dispersed) for easier visualization of all data points. \\(m_{rank}=k_{rank}-1=3-1=2\\)↩ \\(\\Sigma Contrast\\_Codes_{TenuredvAssistant} = -2 + 1 + 1 = 0\\)↩ \\(\\Sigma Contrast\\_Code_{AssociatevProfessor} = 0 -1 + 1 = 0\\)↩ \\(\\Sigma Contrast_{TenuredvAssistant}*Contrast_{AssociatevProfessor} = (2*0) + (1*-1) + (1*1) = 0\\)↩ "],
["one-way-anova.html", "Chapter 10 One-Way ANOVA 10.1 Coding Categorical Variables 10.2 Null and research hypothesis 10.3 Statistical analysis 10.4 Statistical decision 10.5 APA statement 10.6 Visualization", " Chapter 10 One-Way ANOVA Next, let’s go over the one-way ANOVA, which is used when we have two or more independent groups. Specifically, the one-way ANOVA determines if there is a difference in any of the group level means. Given that the analysis can also be used for two independent groups, this analysis could be used instead of the independent samples t-test. For example, let’s say that we were interested in determining if salary was significantly different for professors of different ranks (i.e., assistant professor, associate professor, and professor). For this example, we will use the dataset: datasetSalaries. As mentioned in the independent samples t-test, when dealing with categorical predictors, it is always good idea to check the coding scheme of the categorical variable. contrasts(datasetSalaries$rank) ## 2 3 ## Assistant Professor 0 0 ## Associate Professor 1 0 ## Professor 0 1 10.1 Coding Categorical Variables When coding categorical variables, the number of contrasts (\\(m\\)) is equal to \\(k-1\\) where \\(k\\) is the number of levels (or factors) of an IV. In our example, rank has 3 levels (i.e., Assistant Professor, Associate Professor, and Professor) and thus has 2 contrasts.12 This is the reason why in a one-way ANOVA, we have \\(k-1\\) between degrees of freedom. Thus, contrasts are always being performed, either explicitly if told or using a software package’s default coding scheme (e.g., dummy coding for R) when testing a main effect of a categorical IV. 10.1.1 Dummy Coding Since rank has not yet been explicitly defined by us yet, rank is dummy coded (i.e., for each contrast, a level is coded as 1 and all other levels are coded as 0). In dummy coding, one level will always be coded as 0 in both contrasts. By default, the first level of the categorical IV in alphabetical order will be coded as 0. Thus, in our example, Assistant Professor, which is the first level of rank in alphabetical order is coded as 0. When interpreting dummy coding contrasts, for each contrast, it is the level that is coded as 1 compared to the level that is coded as 0 in both contrasts. In our dummy coding scheme, the first contrast is comparing Associate Professor, which is coded as 1 in the first contrast, to Assistant Professor, which is coded as 0 in both contrasts. The second contrast is comparing Professor, which is coded as 1 in the second contrast, to Assistant Professor, which is again coded as 0 in both contrasts. 10.1.2 Orthogonal Coding However, a priori orthogonal contrast coding is preferred because this coding scheme allows us to create our own a priori contrasts of interest to answer more specific questions. For example, let’s say that in addition to determining if there is a difference in salary of professors based on their rank that we were also interested in more specific questions of interest such as: Determining if there is a difference in salaries of professors that are tenured (Associate Professor and Professor) compared to those that are untenured (Assistant Professor). Determining if there is a difference in salaries of professors that are Associate Professors compared to Professors. contrasts(datasetSalaries$rank) &lt;- cbind( TenuredvAssistant = c(-2, 1, 1), AssociatevProfessor = c(0, -1, 1) ) To make a coding scheme orthogonal, we need to make sure that 1) each contrast equals zero and that 2) the sum of each contrast product is 0. For example, to make the tenured vs. untenured professor contrast, we can code Assistant Professor as -2 and both Associate Professor and Professor as 1 and assign it the name TenuredvAssistant. It’s best to use signs that go in the direction of our prediction so that it’s reflected in our estimates. Since we expect that tenured professors have higher salaries, tenured professors are assigned the positive value and assistant professors are assigned the negative value. Notice that the sum of the TenuredvAssistant contrast also adds up to 0.13 To make the next contrast of Associate Professor compared to Professor, we can code Assistant Professor as 0, Associate Professor as -1, and Professor as 1 and give it the name AssociatevProfessor. Notice again that the sum of the AssociatevProfessor contrast adds up to 0.14 Since both contrasts sum to zero, we can now check if the sum of products for each level across contrasts equals 0. In other words, we multiply the values across each contrast pair for each level and add them together. For example, the product across the contrasts of Assistant Professor is \\(2*0 = 0\\), Associate Professor is \\(1*-1=-1\\), and Professor is \\(1*1=1\\). If we add them together, we get \\(0-1+1=0\\).15 To take orthogonal contrast coding a step further and make our estimates readily interpretable as the mean difference of that contrast, we can ensure that the difference of each contrast is 1. To do this, we can divide each contrast code by the number of values that are not coded as 0 for each contrast. For example, since all 3 codes of the TenuredvAssistant contrast are not equal to 0, we can divide by 3. Similarly, for the AssociatevProfessor contrast, we can divide each contrast code by 2 since 2 of the contrast codes are not equal to 0. Note that this is a rule of thumb and should be verified if contrasts are more complex. contrasts(datasetSalaries$rank) &lt;- cbind( TenuredvAssistant = c(-2 / 3, 1 / 3, 1 / 3), AssociatevProfessor = c(0, -1 / 2, 1 / 2) ) contrasts(datasetSalaries$rank) ## TenuredvAssistant AssociatevProfessor ## Assistant Professor -0.6666667 0.0 ## Associate Professor 0.3333333 -0.5 ## Professor 0.3333333 0.5 10.2 Null and research hypothesis 10.2.1 Traditional approach \\(H_0: \\mu_{Assistant\\_Professors} = \\mu_{Associate\\_Professors} = \\mu_{Professors}\\) \\(H_1: \\mu_{Assistant\\_Professors} \\ne \\mu_{Associate\\_Professors} \\ne \\mu_{Professors}\\) or \\(\\mu_{Assistant\\_Professors} \\ne \\mu_{Associate\\_Professors} = \\mu_{Professors}\\) or \\(\\mu_{Assistant\\_Professors} \\ne \\mu_{Professors} = \\mu_{Associate\\_Professors}\\) or \\(\\mu_{Associate\\_Professors} \\ne \\mu_{Professors} = \\mu_{Assistant\\_Professors}\\) The null hypothesis states that there is no difference in salaries between professors of different ranks. The research hypothesis is stating that the salary of least one rank of professors is different than the others. Thus, the multiple options for a research hypothesis. 10.2.2 GLM approach \\(Model: Salary = \\beta_0 + \\beta_1*TenuredvAssistant + \\beta_2*AssociatevProfessor + \\varepsilon\\) \\(H_0: \\beta_1 = \\beta_2 = 0\\) \\(H_1: \\beta_1 \\ne 0\\) and/or \\(\\beta_2 \\ne 0\\) In the model, we now have both contrasts as predictors. The main effect of rank is testing both predictors of TenuredvAssistant and AssociatevProfessor. Given our orthogonal contrast coding scheme, the intercept (\\(\\beta_0\\)) is now the mean 9-month academic salary of professors on average across rank. The slope of TenuredvAssistant (\\(\\beta_1\\)) represents the mean difference in the 9-month academic salary for professors that are tenured compared to assistant professors. The slope of AssociatevProfessor (\\(\\beta_2\\)) represents the mean difference in the 9-month academic salary for professors that are associate professors compared to professors. Thus, the null hypothesis states that there is a difference in salary of tenured versus untenured professors and there is also no difference in salary of associate professors compared to professors. In other words, there is no difference in salary based on a professor’s rank. The alternative hypothesis states that there is a difference in either the salary of tenured versus untenured professors or associate professor versus professor, or both. If only one contrast is significant, then that contrast must be strong enough to mask the non-significance of the other contrast. Given the vagueness of the research hypothesis (i.e., the significance can be a single contrast or both contrasts), we can look at each individual slope to see which is actually significant since we explicitly defined its associated contrasts beforehand. Thus, our null and research hypotheses for these specific questions would be the following for each slope: \\[H_0: \\beta_1 = 0\\] \\[H_1: \\beta_1 \\ne 0\\] \\[H_0: \\beta_2 = 0\\] \\[H_1: \\beta_2 \\ne 0\\] Each of these null and research hypothesis pairs are also known as 1 degree of freedom tests since we are only testing 1 contrast at a time. 10.3 Statistical analysis 10.3.1 Traditional approach To perform a traditional One-Way ANOVA, we can use the aov() function. The first argument in the aov() function is the formula and the second argument is the name of the dataset. Notice, that in the formula, we do not have to specify both contrasts because we have already applied the coding scheme directly to the categorical IV of rank. model &lt;- aov(salary ~ rank, datasetSalaries) Anova(model, type = c(&quot;III&quot;)) ## Anova Table (Type III tests) ## ## Response: salary ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 2.6481e+12 1 4741.08 &lt; 2.2e-16 *** ## rank 1.4323e+11 2 128.22 &lt; 2.2e-16 *** ## Residuals 2.2007e+11 394 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Traditionally, if the main of effect of a categorical IV was significant, we would perform a post-hoc test (e.g., Tukey’s Honest Significant Difference [HSD]). In our case, rank is significant as the p-value of 2.2e-16 is less than our alpha of 0.05 and we would perform a post-hoc test to determine where the difference in salary actually comes from. To perform Tukey’s HSD, we could use the TukeyHSD function and provide the ANOVA analysis of the model as the input. The Tukey HSD essentially performs multiple independent samples t-tests of all possible pairs of the levels of a categorical IV but uses the error term from the ANOVA analysis in its calculation. TukeyHSD(model) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = salary ~ rank, data = datasetSalaries) ## ## $rank ## diff lwr upr ## Associate Professor-Assistant Professor 13100.45 3382.195 22818.71 ## Professor-Assistant Professor 45996.12 38395.941 53596.31 ## Professor-Associate Professor 32895.67 25154.507 40636.84 ## p adj ## Associate Professor-Assistant Professor 0.0046514 ## Professor-Assistant Professor 0.0000000 ## Professor-Associate Professor 0.0000000 Given that this post-hoc test compares all pairs of levels without any a priori assumptions (similarly, for other post-hoc tests), is another reason to use the a priori orthogonal contrast coding scheme. 10.3.2 GLM approach Notice, that in the formula, we do not have to specify both contrasts because we have already applied the coding scheme directly to the categorical IV of rank. model &lt;- lm(salary ~ 1 + rank, datasetSalaries) Anova(model, type = c(&quot;III&quot;)) ## Anova Table (Type III tests) ## ## Response: salary ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 2.6481e+12 1 4741.08 &lt; 2.2e-16 *** ## rank 1.4323e+11 2 128.22 &lt; 2.2e-16 *** ## Residuals 2.2007e+11 394 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that in both analyses, the F-statistic of 128.22 with 2 between degrees of freedom (df), 394 within degrees of freedom (df), and p-value of 2.2e-16 are identical. summary(model) ## ## Call: ## lm(formula = salary ~ 1 + rank, data = datasetSalaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -68972 -16376 -1580 11755 104773 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 100475 1459 68.855 &lt;2e-16 *** ## rankTenuredvAssistant 29548 3323 8.892 &lt;2e-16 *** ## rankAssociatevProfessor 32896 3290 9.997 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23630 on 394 degrees of freedom ## Multiple R-squared: 0.3943, Adjusted R-squared: 0.3912 ## F-statistic: 128.2 on 2 and 394 DF, p-value: &lt; 2.2e-16 The estimate for the (Intercept) represents the mean 9-month academic salary on average across rank. The estimate for the rankTenuredvAssistant represents the mean difference in 9-month academic salary of tenured professors compared to assistant professors. Specifically, the 9-month academic salary of tenured professors was $29,548 more than assistant professors. The estimate for the rankAssociatevProfessor represents the mean difference in 9-month academic salary of associate professors compared to assistant professors. Specifically, the 9-month academic salary of professors was $32,896 more than associate professors. 10.4 Statistical decision Given the p-value of &lt; 2.2e-16 for rank is less than the alpha level (\\(\\alpha\\)) of 0.05, we will reject the null hypothesis. Notice both slopes are also significant, and we will also reject the null hypothesis for each. 10.5 APA statement An Analysis of Variance (ANOVA) using a priori orthogonal contrast coding was performed to test if there was a difference in the 9-month academic salary between 1) tenured (i.e., associate professors and professors) compared to untenured professors (i.e., assistant professors) and 2) associate professors compared to professors. There was a significant main effect of rank on salary, F(2, 394) = 128.22, p &lt; .001. Specific to our contrasts, tenured professors earned significantly more than untenured professors, b = $29,548, t(1, 394) = 8.892, p &lt; .001. Professors also earned significantly more than professors, b = $32,896, t(1, 394) = 9.997, p &lt; .001. 10.6 Visualization 10.6.1 Traditional # calculate descriptive statistics along with the 95% CI dataset_summary &lt;- datasetSalaries %&gt;% group_by(rank) %&gt;% summarize( mean = mean(salary), sd = sd(salary), n = n(), sem = sd / sqrt(n), tcrit = abs(qt(0.05 / 2, df = n - 1)), ME = tcrit * sem, LL95CI = mean - ME, UL95CI = mean + ME ) # plot ggplot(datasetSalaries, aes(rank, salary)) + geom_jitter(alpha = 0.1, width = 0.1) + geom_errorbar(data = dataset_summary, aes(x = rank, y = mean, ymin = LL95CI, ymax = UL95CI), color = &quot;#3182bd&quot;, width = 0.02) + geom_point(dat = dataset_summary, aes(x = rank, y = mean), color = &quot;#3182bd&quot;, size = 3) + labs( x = &quot;Rank&quot;, y = &quot;9-Month Academic Salary (USD)&quot; ) + scale_y_continuous(labels = scales::dollar) + theme_classic() Figure 10.1: A dot plot of the 9-month academic salaries of professors by their rank within the university (i.e., assistant professor, associate professor, and professor). Respectively for each rank, the dot represents the mean salary and the bars represent the 95% CI. Note: The data points are only jittered (dispersed) for easier visualization of all data points. 10.6.2 Orthogonal Contrast Coding datasetSalaries_long &lt;- datasetSalaries %&gt;% mutate(professor = 1:nrow(.), TenuredvAssistant = case_when( rank == &quot;Assistant Professor&quot; ~ -2, rank == &quot;Associate Professor&quot; ~ 1, rank == &quot;Professor&quot; ~ 1), AssociatevProfessor = case_when( rank == &quot;Assistant Professor&quot; ~ 0, rank == &quot;Associate Professor&quot; ~ -1, rank == &quot;Professor&quot; ~ 1) ) %&gt;% select(professor, salary, TenuredvAssistant, AssociatevProfessor) %&gt;% reshape2::melt(., id.vars = c(&quot;professor&quot;, &quot;salary&quot;)) %&gt;% filter(value != 0) %&gt;% rowwise() %&gt;% mutate(rank = case_when( variable == &quot;TenuredvAssistant&quot; &amp;&amp; value == -2 ~ &quot;Assistant&quot;, variable == &quot;TenuredvAssistant&quot; &amp;&amp; value == 1 ~ &quot;Tenured&quot;, variable == &quot;AssociatevProfessor&quot; &amp;&amp; value == 1 ~ &quot;Professor&quot;, variable == &quot;AssociatevProfessor&quot; &amp;&amp; value == -1 ~ &quot;Associate&quot; )) dataset_summary &lt;- datasetSalaries_long %&gt;% group_by(variable, value) %&gt;% summarize( mean = mean(salary), sd = sd(salary), n = n(), sem = sd / sqrt(n), tcrit = abs(qt(0.05 / 2, df = n - 1)), ME = tcrit * sem, LL95CI = mean - ME, UL95CI = mean + ME ) ggplot(datasetSalaries_long, aes(value, salary)) + geom_jitter(alpha = 0.1, width = 0.3) + geom_errorbar(data = dataset_summary, aes(x = value, y = mean, ymin = LL95CI, ymax = UL95CI), color = &quot;#3182bd&quot;, width = 0.05) + geom_line(data = dataset_summary, aes(x = value, y = mean)) + geom_point(data = dataset_summary, aes(x = value, y = mean), color = &quot;#3182bd&quot;, size = 3) + geom_text(aes(label = rank, x = value, y = 0), size = 3) + facet_wrap(~ variable) + labs( x = &quot;Rank&quot;, y = &quot;9-Month Academic Salary (USD)&quot; ) + scale_y_continuous(labels = scales::dollar) + scale_x_continuous(breaks = seq(-3, 3, 1), limits = c(-3,3)) + theme_classic() + theme(strip.background = element_blank()) Figure 10.2: A dot plot of the 9-month academic salaries of professors by their rank within the university. Respectively for each rank, the dot represents the mean salary and the bars represent the 95% CI. Note: The data points are only jittered (dispersed) for easier visualization of all data points. \\(m_{rank}=k_{rank}-1=3-1=2\\)↩ \\(\\Sigma Contrast\\_Codes_{TenuredvAssistant} = -2 + 1 + 1 = 0\\)↩ \\(\\Sigma Contrast\\_Code_{AssociatevProfessor} = 0 -1 + 1 = 0\\)↩ \\(\\Sigma Contrast_{TenuredvAssistant}*Contrast_{AssociatevProfessor} = (2*0) + (1*-1) + (1*1) = 0\\)↩ "],
["factorial-anova.html", "Chapter 11 Factorial ANOVA 11.1 Null and research hypotheses 11.2 Statistical analysis 11.3 Statistical decision 11.4 APA statement 11.5 Visualization", " Chapter 11 Factorial ANOVA The factorial ANOVA, which tests more than one categorical IV, is an extension of the one-way ANOVA, which only tests one categorical IV. We can also think of the one-way ANOVA as a special case of the factorial ANOVA. In addition to testing the main effects of the categorical IV, the factorial ANOVA also tests the interactions of the categorical IVs. For example, let’s say in addition to the question we asked in the independent samples t-test chapter (i.e., are salaries of professors different depending on their discipline?). Let’s also say that we were interested in two more questions: Does the salary of professors depend on their sex? Does the relationship between salary of professors and their discipline also depend on their sex? The first question is the overall effect of a categorical IV, which is known as a main effect. The second question is the interaction of the two main effects (i.e., discipline and sex) and is automatically formed and tested in traditional factorial ANOVA. (Note: Depending on our specific research questions, the interaction term may or not be needed. Thus, the automatic formation of the interaction within the traditional factorial ANOVA may not always be beneficial. Using the GLM approach gives us the flexibility to include or not include interaction terms.) For this example, we will again use the datasetSalaries dataset. Before we continue, let’s again check the contrasts of both variables. contrasts(datasetSalaries$discipline) ## Theoretical ## Applied 0 ## Theoretical 1 contrasts(datasetSalaries$sex) ## Male ## Female 0 ## Male 1 Both of the variables are currently dummy coded. Since we prefer a priori orthogonal contrast code, let’s go ahead and change the contrast for both of these factors. We can code professors in the applied discipline as 0.5 and professors in the theoretical discipline as -0.5 since we think that those in the applied discipline will be earn a higher salary. Similarly, we can code male professors as 0.5 and female professors as -0.5 since we also think that male professors will likely earn a higher salary than female professors. Note: Since each factor is orthogonal within its respective factor, the sum of each contrast pair between the factors is also equal to 0. Thus, we do not need to actually check this; however, we encourage you test this on your own. contrasts(datasetSalaries$discipline) &lt;- cbind(AppliedvTheoretical = c(1 / 2, -1 / 2)) contrasts(datasetSalaries$discipline) ## AppliedvTheoretical ## Applied 0.5 ## Theoretical -0.5 contrasts(datasetSalaries$sex) &lt;- cbind(FemalevMale = c(-1 / 2, 1 / 2)) contrasts(datasetSalaries$sex) ## FemalevMale ## Female -0.5 ## Male 0.5 11.1 Null and research hypotheses Given that there are a total of three questions of interest, there will also be three pairs of null and research hypotheses. 11.1.1 Traditional Approach Main Effect of Discipline \\[H_0: \\mu_{Applied} = \\mu_{Theoretical}\\] \\[H_1: \\mu_{Applied} \\ne \\mu_{Theoretical}\\] The null hypothesis states that there is no difference in a professor’s salary whether they work in an applied discipline compared to a theoretical discipline on average across sex. The research hypothesis states that there is salary difference in professors in the applied discipline compared to the theoretical discipline on average across sex. Main Effect of Sex \\[H_0: \\mu_{Female} = \\mu_{Male}\\] \\[H_1: \\mu_{Female} \\ne \\mu_{Male}\\] The null hypothesis states there is no salary difference in professors that are female compared to those that are male on average across discipline. The research hypothesis states there is a salary difference in professors that are female compared to those that are male on average across discipline. Main Interaction Effect of Discipline by Sex \\[H_0: \\mu_{Female_{Applied}} - \\mu_{Female_{Theoretical}} = \\mu_{Male_{Applied}} - \\mu_{Male_{Theoretical}}\\] \\[H_0: \\mu_{Female_{Applied}} - \\mu_{Female_{Theoretical}} \\ne \\mu_{Male_{Applied}} - \\mu_{Male_{Theoretical}}\\] The null hypothesis states that the effect of discipline (i.e., the difference of theoretical and applied) are the same for males and females. In other words, the relationship between a professor’s salary and their discipline does not also change depending on their sex. The research hypothesis states that the relationship between a professor’s salary and their discipline does change based on their sex. If we were interested in determining if the relationship between sex and salary depended on discipline, we could swap the subscripts so that sex is the subscript of discipline. 11.1.2 GLM Approach \\(Model: Salary = \\beta_0 + \\beta_1*Discipline + \\beta_2*Sex + \\beta_3*Discipline*Sex + \\varepsilon\\) Main Effect of Discipline \\[H_0: \\beta_1 = 0\\] \\[H_1: \\beta_1 \\ne 0\\] The null hypothesis states that the slope associated with discipline is 0. Since the difference of our contrast is 1, the slope represents the mean salary difference between discipline on average across sex. In other words, the null hypothesis states that there is no difference in a professor’s salary based on their discipline on average across sex. The research hypothesis states that there is a difference in a professor’s salary based on their discipline on average across sex. Main Effect of Sex \\[H_0: \\beta_2 = 0\\] \\[H_1: \\beta_2 \\ne 0\\] The null hypothesis states that the slope associated with sex is 0. Since the difference of our contrast is 1, the slope represents the mean salary difference between sex on average across discipline. In other words, the null hypothesis states a professor’s salary is not related to their sex on average across their discipline. The research hypothesis states that a professor’s salary is related to their sex on average across their discipline. Main Interaction Effect of Discipline by Sex \\[H_0: \\beta_3 = 0\\] \\[H_1: \\beta_3 \\ne 0\\] The null hypothesis states that the slope associated with the interaction of discipline and sex is 0. Slopes associated with interactions can be thought of as changes in the slope associated with one IV included in the interaction for every 1 unit of another IV included in the interaction. In other words, if we calculated the slope between salary and discipline for just female professors and then calculated the slope between salary and discipline for just male professors, \\(\\beta_3\\) represents the difference in these slopes. Since the difference of our contrast for sex is 1, the slope represents the difference in the slope between salary and discipline between males and females. The null hypothesis states that the relationship between a professor’s salary and their discipline does not change based on their sex. The research hypothesis states that the relationship between a professor’s salary and their discipline does change based on their sex. 11.2 Statistical analysis 11.2.1 Traditional approach To perform a traditional factorial ANOVA, we can again use the aov() function. In the first argument, which is the formula, we can enter the DV followed by the tilde (~) and then the IVs along with its interaction. We can indicate an interaction by placing a colon (:) between the two IVs we want to interact. Specifically, this will create a new interaction term by multiplying the two IVs for each subject. model &lt;- aov(salary ~ discipline + sex + discipline:sex, datasetSalaries) We can also more concisely place an asterisk (*) between the two IVs to perform the exact same test (i.e., a test of an interaction along with a test for each main effect included in the interaction term). We prefer this form as we prefer simplicity. model &lt;- aov(salary ~ discipline * sex, datasetSalaries) Anova(model, type = c(&quot;III&quot;)) ## Anova Table (Type III tests) ## ## Response: salary ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 1.6139e+12 1 1834.2735 &lt; 2.2e-16 *** ## discipline 7.9856e+09 1 9.0759 0.002758 ** ## sex 7.4307e+09 1 8.4453 0.003867 ** ## discipline:sex 1.7395e+09 1 1.9770 0.160492 ## Residuals 3.4579e+11 393 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Main Effect of Discipline We can see from the F-statistic (F value) that the ability for discipline to explain salary is about 9.0759 times greater than its inability to explain salary, which is very large. Main Effect of Sex We can also see that the ability for sex to explain salary is about 8.4453 times greater than its inability to explain salary, which is also very large. Interaction Effect of Discipline by Sex However, the ability for the interaction between sex and discipline to explain salary is about 1.9770 times greater than its inability to explain salary, which is pretty small (close to 1). 11.2.2 GLM approach model &lt;- lm(salary ~ 1 + discipline * sex, datasetSalaries) Anova(model, type = c(&quot;III&quot;)) ## Anova Table (Type III tests) ## ## Response: salary ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 1.6139e+12 1 1834.2735 &lt; 2.2e-16 *** ## discipline 7.9856e+09 1 9.0759 0.002758 ** ## sex 7.4307e+09 1 8.4453 0.003867 ** ## discipline:sex 1.7395e+09 1 1.9770 0.160492 ## Residuals 3.4579e+11 393 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that in both analyses, the statistics are identical for each research question: Main Effect of Discipline When we look at the row for discipline, the F-statistic of 5.41 with 1 between degrees of freedom (df), 393 within degrees of freedom (df), and p-value of .020 are identical. Main Effect of Sex When we look at the row for sex, the F-statistic of 1.22 with 1 between degrees of freedom (df), 393 within degrees of freedom (df), and p-value of .270 are identical. Main Interaction Effect of Discipline by Sex When we look at the row for the interaction of discipline and sex (discipline:sex) the F-statistic of 1.98 with 1 between degrees of freedom (df), 393 within degrees of freedom (df), and p-value of .160 are identical. summary(model) ## ## Call: ## lm(formula = salary ~ 1 + discipline * sex, data = datasetSalaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -52900 -23149 -5419 20505 112785 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 107440 2509 42.828 ## disciplineAppliedvTheoretical 15115 5017 3.013 ## sexFemalevMale 14580 5017 2.906 ## disciplineAppliedvTheoretical:sexFemalevMale -14109 10034 -1.406 ## Pr(&gt;|t|) ## (Intercept) &lt; 2e-16 *** ## disciplineAppliedvTheoretical 0.00276 ** ## sexFemalevMale 0.00387 ** ## disciplineAppliedvTheoretical:sexFemalevMale 0.16049 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29660 on 393 degrees of freedom ## Multiple R-squared: 0.0482, Adjusted R-squared: 0.04094 ## F-statistic: 6.634 on 3 and 393 DF, p-value: 0.0002217 The estimates are slightly different now with the addition of another IV and its interaction, so we will interpret the estimate of each predictor. The intercept (\\(b_0\\)) still represents the value of the DV when all IVs are 0. Since the codes of our categorical IVs are orthogonal (i.e., 0 lies in the middle of the categorical IV codes), the (Intercept) represents the 9-month academic salary of professors on average across discipline and sex. We don’t need to mention when the interaction is also 0 because if the two variables within the interaction are zero, then the interaction must also equal 0. The estimate for the slope associated with discipline (\\(b_1\\)) represents the change in salary for every 1 unit increase in discipline when sex equals 0. Since the difference in the coding scheme of discipline is 1 and since 0 lies in-between the two codes for sex, the slope represents the mean salary difference of professors in applied compared to theoretical discipline on average across sex. Specifically, it is estimated that professors in the theoretical discipline earn $15,115 less than professors in an applied discipline on average across sex. We need to say on average across sex because we have an interaction term and are allowing for the slopes to vary at different levels of each IV. The estimate for the slope associated with sex (\\(b_2\\)) is interpreted similarly to the slope associated with discipline. The estimate for the slope associated with sex represents the change in salary for every 1 unit increase in sex when discipline equals 0. Again, since the difference in the coding scheme for sex is 1 and since 0 lies in-between the two codes for discipline, the slope represents the mean salary difference of professors that are female compared to professors that are male on average across discipline. Specifically, it is estimated that professors that are male earn $14,580 more than professors that are female on average across discipline. The estimate for the slope associated with the interaction of discipline and sex (\\(b_3\\)) represents the change in the slope of salary and discipline for every 1 unit increase in sex. Since the difference in the coding scheme of sex is 1, the slope represents the sex difference of the slope of salary and discipline. Specifically, the slope of salary and discipline is estimated to decrease by $14,109 for males compared to females In other words, the negative relationship between salary and discipline is expected to strengthen for males compared to females. 11.3 Statistical decision Let’s examine the statistical decision for each research question. Main Effect of Discipline Given the p-value of .020 is less than the alpha level (\\(\\alpha\\)) of 0.05, we will reject the null hypothesis. Main Effect of Sex Given the p-value of .270 is greater than the alpha level (\\(\\alpha\\)) of 0.05, we will retain (or fail to reject) the null hypothesis. Interaction Effect of Discipline by Sex Given the p-value of .160 is greater than the alpha level (\\(\\alpha\\)) of 0.05, we will retain (or fail to reject) the null hypothesis. 11.4 APA statement A factorial Analysis of Variance (ANOVA) was performed to test if 1) the salary of professors was different depending on their discipline, 2) the salary of professors was different depending on their sex, and 3) the relationship between salary of professors and their sex depending on their discipline. There was significant main effect of discipline on salary, F(1,393) = 9.08, p = .003. There was also a significant main effect of sex on salary, F(1,393) = 8.45, p = .004. However, the relationship between salary of professors and their sex did not change depending on their discipline, F(1,393) = 1.98, p = .160. 11.5 Visualization # calculate descriptive statistics along with the 95% CI dataset_summary &lt;- datasetSalaries %&gt;% group_by(discipline, sex) %&gt;% summarize( mean = mean(salary), sd = sd(salary), n = n(), sem = sd / sqrt(n), tcrit = abs(qt(0.05 / 2, df = n - 1)), ME = tcrit * sem, LL95CI = mean - ME, UL95CI = mean + ME ) %&gt;% mutate(discipline_code = ifelse(discipline == &quot;Applied&quot;, 1 / 2, -1 / 2)) # plot datasetSalaries %&gt;% mutate(discipline_code = ifelse(discipline == &quot;Applied&quot;, 1 / 2, -1 / 2)) %&gt;% ggplot(., mapping = aes(discipline_code, salary, group = sex, shape = sex, color = sex)) + geom_jitter(alpha = 0.2, width = 0.2) + geom_point( data = dataset_summary, aes(x = discipline_code, y = mean), size = 3 ) + geom_line(data = dataset_summary, aes(x = discipline_code, y = mean)) + geom_errorbar( data = dataset_summary, aes(x = discipline_code, y = mean, ymin = LL95CI, ymax = UL95CI), width = 0.02 ) + labs( x = &quot;Discipline&quot;, y = &quot;9-Month Academic Salary (USD)&quot;, color = &quot;Sex&quot;, shape = &quot;Sex&quot; ) + theme_classic() + scale_x_continuous(breaks = seq(-2, 2, 1 / 2), limits = c(-1, 1)) + scale_y_continuous(labels = scales::dollar) + annotate(geom = &quot;text&quot;, x = -.5, y = 0, label = &quot;Applied&quot;, size = 4) + annotate(geom = &quot;text&quot;, x = .5, y = 0, label = &quot;Theoretical&quot;, size = 4) Figure 11.1: A dot plot of the 9-months salaries of professors by their discipline and sex. Respectively for each discipline and sex, the dot represents the mean and the bars represent the 95% CI. "],
["glm-approach-summary.html", "Chapter 12 GLM Approach Summary", " Chapter 12 GLM Approach Summary To summarize, the introductory univariate statistics typically taught and learned in psychology courses can be applied within the general linear model (GLM) context by taking the concise and unified form of: \\[Y = \\beta X+\\varepsilon\\] We showed how each of the traditional univariate statistics in psychology can be written in a GLM format (i.e., ordinary least squares [OLS] regression). We also showed how each of the traditional univariate statistics can be performed using the lm() function. Within this GLM framework, we can continue to analyze more complicated models with different combinations of IVs such as using multiple continuous IVs (multiple regression) or mixing categorical and continuous variables (traditionally ANCOVA). We can also expand this to repeated measures design such as repeated-measures ANOVA, repeated-measures ANCOVA, and multi-level modeling. We again highly recommend the book Data Analysis by Judd, McClelland, &amp; Ryan for an intuitive and fundamental understanding of these statistics. We hope that we have guided and convinced you how these analyses can be thought of and analyzed in the GLM format. More importantly, we also hope that you will continue implement this GLM framework and mindset so that you can readily expand to more complicated statistical models and analyses. "],
["references.html", "References", " References "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements I would like to thank: Maria Boylan for pushing me to explain more and turning a blog into this book. Linh Lazarus (@praxling) for editing an earlier version of this when it was only a blog post. \\(\\dagger\\) "],
["authors.html", "Authors Ekarin Eric Pongpipat, M.A. Giuseppe Miranda, M.S. Matthew J. Kmiecik, B.S.", " Authors Ekarin Eric Pongpipat, M.A. Ekarin Eric Pongpipat is a Ph.D. student studying cognition and neuroscience at the University of Texas at Dallas. He obtained his undergraduate degree in psychology at California State University, Northridge and his master’s degree in psychology at San Diego State University. Ekarin has over 5 years of teaching and tutoring experience within statistics. He is also interested in understanding how neural systems change with age. Giuseppe Miranda, M.S. Giuseppe Miranda is a Ph.D. student studying cognition and neuroscience at the University of Texas at Dallas. Matthew J. Kmiecik, B.S. Matt Kmiecik is a PhD candidate studying cognition and neuroscience at The University of Texas at Dallas. He obtained his undergraduate degree in Psychology: Natural Sciences at Loyola University Chicago. He is interested in human reasoning, thinking, and how language subserves complex cognitive processing. In his spare time, Matt enjoys performing hockey analytics, reading, and bouldering. "]
]
